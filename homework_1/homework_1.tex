%This is my super simple Real Analysis Homework template

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage[]{amsmath}

\title{Homework 1}
\author{Chu Hai Nam MSSV: 2370189}
\date\today
%This information doesn't actually show up on your document unless you use the maketitle command below

\begin{document}
\maketitle %This command prints the title based on information entered above

\section*{Section 3.2}
Standard notations and common functions

\subsection*{Problem 3.2.3}
Is $2^{n+1} = O(2^n)$ ? Is $2^{2n} = O(2^n)$ ?
\begin{proof}
    $ $\newline
    $ $\newline
    We determine positive constants $c$ and $n_0$ such that $0\le2^{n+1}\le c2^n$ for all $n\ge n_0$. \\
    Since $2^{n+1}=2\cdot2^n$, we can pick $c=2$ and $n_0=1$. \\
    So $2^{n+1}=O(2^n)$. \\
    $ $\newline
    We determine positive constants $c$ and $n_0$ are positive constants satisfying $0\le2^{2n}\le c2^n$ for all $n\ge n_0$. \\
    Then $2^{2n}=2^n\cdot2^n\le c2^n$, which implies that $c\ge2^n$. \\
    We would not choose the $c$, because $2^n$ becomes arbitrarily large as $n$ gets large. \\
    So, $2^{2n}\ne O(2^n)$. \\
\end{proof}

\newcommand{\Tworst}{T_{worst}(n)}
\newcommand{\Tbest}{T_{best}(n)}
\subsection*{Problem 3.2.5}
Prove that the running time of an algorithm is $ \Theta(g(n)) $ if and only if its worst-case
running time is $ O(g(n)) $ and its best-case running time is $ \Omega(g(n)) $.
\begin{proof}
    $ $\newline
    $ $\newline
    For the input size $n$ with $T(n)$ be the running time of an algorithm, 
    and let $\Tworst$ and $\Tbest$ be its worst-case running time and its best-case running time, respectively. \\
    $ $\newline
    Suppose that $T(n)=\Theta(g(n))$. \\
    For all input size $n$, there is one worst case and one best case for the algorithm 
    in terms of time required to run on these inputs. \\
    Since $T(n)$ describes the running time for \emph{all} cases, it also describes the 
    running time for the worst case and the best case. \\
    Thus, $\Tworst=\Theta(g(n))$ and $\Tbest=\Theta(g(n))$, and by Theorem 3.1 \\
    We have that $\Tworst=O(g(n))$ and $\Tbest=\Omega(g(n))$. \\
    $ $\newline
    Suppose that $\Tbest=\Omega(g(n))$ and $\Tworst=O(g(n))$. \\
    Let $c_1$ and $n_1$ be positive constants such that $0\le c_1g(n)\le\Tbest$ for all $n\ge n_1$, \\
    Let $c_2$ and $n_2$ be positive constants such that $0\le\Tworst\le c_2g(n)$ for all $n\ge n_2$. \\
    For any input size $n$, $\Tbest\le T(n)\le\Tworst$.
    Then for all $n\ge\max{n_1,n_2}$,
    \[
        0 \le c_1g(n) \le \Tbest \le T(n) \le \Tworst \le c_2g(n),
    \]
    So $T(n)=\Theta(g(n))$.
\end{proof}

\section{5.2-1}

Hiring exactly one time if the best candidate is presented first. There are $(n-1)!$ orderings with the best candidate first, therefore, it is with probability $\frac{(n-1)!}{n!} = \frac{1}{n}$ that you only hire once. You will hire exactly n times if the candidates are presented in increasing order. This fixes the ordering to a single one,and so this will occur with probability $\frac{1}{n!}$

\section{5.2-5}
 We want to determine the expected number of fixed points in a random permutation.A fixed point of a permutation $\pi$ is a value $i$ for which $\pi(i) = i$.\\
 We could enumerate all $n!$ permutations, count the total number of fixed points, and divide by $n!$ to determine the average number of fixed points per permutation. This would be a painstaking process, and the answer would turn out to be 1. We can use indicator random variables, however, to arrive at the same answer much more easily.\\
Define a random variable X that equals the number of customers that get back their own hat, so that we want to compute $E[X]$.\\
For $i=1,2,...n$ define the indicator random variable\\
$X_i = I$\{customer i gets back his own hat\}\\
Then $X = X_1+X_2+...+X_n$
Since the ordering of hats is random, each customer has a probability of $\frac{1}{n}$ of getting back their own hat. \\
In other words, $P_r\{X_i=1\}=\frac{1}{n}$ , which implies that $E[X_i]=\frac{1}{n}$\\
Therefore,\\
$E[X_i]=E[\displaystyle\sum_{i=1}^{n}X_i]=\displaystyle\sum_{i=1}^{n}E[X_i]=\displaystyle\sum_{i=1}^{n}\frac{1}{n}=1$
\\
Thus, we expect that exactly 1 customer gets back their own hat

\section*{Section 2.2}
%
\subsection*{Problem 6}
Blah
\subsection*{Problem 7}
Blah
\subsection*{Problem 10}
Blah

\end{document}